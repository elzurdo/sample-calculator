import matplotlib.pyplot as plt
import streamlit as st

from utils_viz import plot_success_rates_methods, \
    plot_boundary_true_false_positive_rates

from utils_stats import observed_success_correctness, SUCCESS_RATE_BOUNDARY

ci_fraction = 0.95
observed_success_percent = st.number_input('Audit Success Rate (in %)', value=98., min_value=0.01,max_value=99.)

observed_success_rate = observed_success_percent / 100.

sample_size = st.number_input('Audit Size', value=100, min_value=100,
                              max_value=10000)

mfpr_percent = st.number_input('Max False Positive Rate committed to (Max FPR) in (%)', value=1., min_value=0.01,max_value=5.)
mfpr_rate = mfpr_percent / 100.


less_equal = r"""$$\le$$"""
text_short_mfpr = f"""
*A model is determined safe if the **Audit FPR**{less_equal}{mfpr_percent:0.2f}%.   
This guarantees that for every 1,000 similar decisions (of "this model is safe"), we consider a maximum of
{mfpr_rate * 1000.:0.0f} incorrect decisions to be acceptable.*
"""

text_short_mfpr


success_rate_boundary = st.sidebar.number_input('Model Min Success Rate', value=0.93, min_value=0.9,
                                     max_value=0.99)



with st.beta_expander('Visualise Result'):
    plot_boundary_true_false_positive_rates(observed_success_rate, sample_size, success_rate_boundary=success_rate_boundary)
    #plot_audit_success_rate_tpr_tnr(audit_success_rate, audit_size, success_rate_boundary=success_rate_boundary)
    plot_success_rates_methods(observed_success_rate, sample_size, ci_fraction, ci_type="HDI", legend_title= None)
    st.pyplot(plt.gcf())


observation_result = observed_success_correctness(observed_success_rate, sample_size, generator_success_rates=None,
                                 success_rate_boundary=success_rate_boundary,
                                 tpr_method="flat_min", min_ab=0.5)

boundary_success_bool =  observation_result['rate_type'] == 'success'

if boundary_success_bool:
    str_over_under = """**over**"""
    observed_fpr = observation_result['false_rate']
    # audit passes boundary
    mfpr_success_bool =  observation_result['false_rate'] <= mfpr_rate

    if mfpr_success_bool:
        # with a FPR smaller than the maximum
        audit_result_str = f""" The model that generated this 
                            audit result **may be considered safe**! ðŸŽ‰ðŸŽˆðŸŽŠ 

**Reason**  
The result indicates that **Audit FPR** is **smaller** than **Max FPR** 
                            ({observation_result['false_rate']* 100.:0.2f}% 
                            {less_equal}{mfpr_rate * 100.:0.2f}%). 
                            """
    else:
        # with a FPR smaller than the maximum

        audit_result_str = f""" The model that generated this 
                                    audit result **may NOT be considered safe**. ðŸ˜¦ 
        
**Reason**  
The result indicates that **Audit FPR** is **larger** than **Max FPR** 
({observation_result['false_rate'] * 100.:0.2f}%>{mfpr_rate * 100.:0.2f}%). 


Considering this model as "safe" would risk increasing the FPR to above the committed {mfpr_rate * 100.:0.2f}% (i.e, letting more unsafe models pass).

**Suggested Actions**  
* Explore the reason for the high unsafe rate and fix the model.  
* Collect more annotated cases for further justification. Note that the Max FPR must remain at {mfpr_rate * 100.:0.2f}% or lower.  
"""
else:
    str_over_under = """**equal or under**"""
    observed_fpr = observation_result['true_rate']
    audit_result_str = f"""The model that generated the audit sample **may not be considered safe**. ðŸ˜¦   

**Reason**  
The Audit Success Rate of {observed_success_rate * 100:0.1f}%  is lower than the threshold of {success_rate_boundary * 100:0.1f}%.  ðŸ˜± 

**Suggested Action**  
Explore the reason for the high unsafe rate and fix the model. 

"""



interpretation_boundary_success_text = f"""  
**Result**   
An audit of size 
{sample_size} 
with a success rate of 
{observed_success_rate * 100:0.1f}% 
has an **Audit FPR** of 
{observed_fpr * 100:0.1f}%. In other words, there is a  {(1.-observed_fpr) * 100:0.1f}%
probability that this sample was generated by a model with a success rate of {str_over_under} 
{success_rate_boundary * 100:0.1f}%.

**Conclusion**   
{audit_result_str}


"""

interpretation_boundary_success_text


text_expanded_mfpr = f"""
By committing to a **Max False Positive Rate (Max FPR)** of 
{mfpr_percent}%, results with with a higher **Audit FPR** will be considered not safe. 

This commitment guarantees that for every 1,000 audits, we should expect and average of 
{mfpr_rate * 1000.:0.1f} to be False Positives.
"""

with st.beta_expander("""Why should we commit to Max FPR?"""):
    st.write(text_expanded_mfpr)


